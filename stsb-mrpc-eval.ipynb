{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c847f6a1",
   "metadata": {},
   "source": [
    "## Text Similarity Feature\n",
    "\n",
    "The goal here is to check the performance of openai and cohere embedding model on some specific datasets on \n",
    "textual similarity and to detect plagiarism.\n",
    "\n",
    "### Reporting results\n",
    "#### STSB dataset (1500 datasets. Very short sentences)\n",
    "> Note: Time eval not that accurate as it includes time it takes for dataset to run. I made sure to restart kernel between runs.\n",
    "\n",
    "__text-embedding-3-small__\n",
    "\n",
    "time = 38secs (0.63mins); correlation = 0.88\n",
    "\n",
    "__text-embedding-3-large__\n",
    "\n",
    "time = 34secs (0.57mins); correlation = 8.88\n",
    "\n",
    "#### MRPC dataset (408 datasets. somewhat short sentences as well)\n",
    ">Note this eval does not include time required to load dataset\n",
    "\n",
    "After eyeballing the MRPC dataset, it is a really tricky dataset because any model that really does well on the dataset would have to be good at making a really good distinction between paraphrased entry(in that text2 is a paraphrased text that stays on topic) and semantic entry(in that text2 is a semantic text that good number of words in texts but generally does not stay on topic).\n",
    "\n",
    "##### text-embedding-3-small\n",
    "\n",
    "Scaled similarity embeddings have length 408 same as dataset. hence each score represents an entry in the dataset.\n",
    "\n",
    "time = 11.3 secs (0.19 minutes); \n",
    "mean-similarity (equivalent pair) = 0.93\n",
    "mean-similarity (non-equivalent pair) = 0.87\n",
    "separation between the two = 0.06\n",
    "\n",
    "**classification metrics**\n",
    "precision = 0.7319\n",
    "recall = 0.9785\n",
    "f1-Score = 0.8374\n",
    "\n",
    "**ranking metrics**\n",
    "ROC-AUC = 0.7711\n",
    "PR-AUC = 0.8755\n",
    "\n",
    "##### text-embedding-3-large\n",
    "\n",
    "Scaled similarity embeddings have length 408 same as dataset. hence each score represents an entry in the dataset.\n",
    "\n",
    "time = 14.85 seconds (0.25 minutes); \n",
    "mean-similarity (equivalent pair) = 0.93\n",
    "mean-similarity (non-equivalent pair) = 0.87\n",
    "separation between the two = 0.06\n",
    "\n",
    "**classification metrics**\n",
    "precision = 0.7549\n",
    "recall = 0.9606\n",
    "f1-Score = 0.8454\n",
    "\n",
    "**ranking metrics**\n",
    "ROC-AUC = 0.7816\n",
    "PR-AUC = 0.8809\n",
    "\n",
    "### Summary\n",
    "- For STSB, openai embedding small and large achieved a correlation of `0.88` with ground-truth labels on STSB dataset with no clear winner.\n",
    "- For MRPC, openai embedding small and large have about the same scores. So going forward. There is not much benefit testing on `text-embedding-3-large`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f3ea5",
   "metadata": {},
   "source": [
    "## Evaluate OPENAI embed model on STSB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b17ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load STSB dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load STSB validation set for a quick evaluation\n",
    "dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
    "\n",
    "# The validation set has 1500 pairs\n",
    "\n",
    "# Get all pairs and true scores\n",
    "sents1 = dataset[\"sentence1\"]\n",
    "sents2 = dataset[\"sentence2\"]\n",
    "scores = dataset[\"score\"]  # Ground truth similarity scores (float: 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9e327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching embeddings for sentence1...\n",
      "Fetching embeddings for sentence2...\n",
      "OpenAI embedding on STSB (validation): Spearman correlation = 0.8775\n",
      "\n",
      "Total execution time: 33.93 seconds (0.57 minutes)\n"
     ]
    }
   ],
   "source": [
    "# @title Evaluate openai model on STSB dataset on huggingface\n",
    "\n",
    "# Evaluate openai embedding on STSB dataset on huggingface\n",
    "# Url https://huggingface.co/datasets/sentence-transformers/stsb\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key from .env file\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file. Please create a .env file with OPENAI_API_KEY=your_key\")\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    # Handles too-long texts by truncating\n",
    "    if len(text) > 8191:\n",
    "        text = text[:8191]\n",
    "    response = client.embeddings.create(model=model, input=[text])\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def batch_get_openai_embeddings(texts, model=\"text-embedding-ada-002\", batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        response = client.embeddings.create(model=model, input=batch)\n",
    "        batch_embeds = [item.embedding for item in response.data]\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "print(\"Fetching embeddings for sentence1...\")\n",
    "embeds1 = batch_get_openai_embeddings(sents1, model=\"text-embedding-3-large\")\n",
    "print(\"Fetching embeddings for sentence2...\")\n",
    "embeds2 = batch_get_openai_embeddings(sents2, model=\"text-embedding-3-large\")\n",
    "\n",
    "# Cosine similarity function\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compute similarity scores\n",
    "pred_similarities = [\n",
    "    cosine_similarity(e1, e2) for e1, e2 in zip(embeds1, embeds2)\n",
    "]\n",
    "# Scale cosine similarity (-1..1) to (0..1) to match the normalized STSB scoring\n",
    "pred_similarities_scaled = [(sim + 1) / 2 for sim in pred_similarities]\n",
    "\n",
    "# Evaluate with Spearman correlation coefficient\n",
    "spearman_corr, _ = spearmanr(pred_similarities_scaled, scores)\n",
    "print(f\"OpenAI embedding on STSB (validation): Spearman correlation = {spearman_corr:.4f}\")\n",
    "\n",
    "# End timing and print duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal execution time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6052998",
   "metadata": {},
   "source": [
    "## Evaluate OPENAI embed model on MRPC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0402c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Load MRPC dataset\n",
    "# url https://huggingface.co/datasets/SetFit/mrpc\n",
    "from datasets import load_dataset\n",
    "\n",
    "# The validation set has 408 pairs\n",
    "dataset = load_dataset(\"SetFit/mrpc\", split=\"validation\")\n",
    "\n",
    "# There is no score here only binary label (`0` and `1`)\n",
    "# 1 means equivalent(SIMILAR)\n",
    "# 0 means non-equivalent(DISSIMILAR)\n",
    "# The two texts are labeled text1 and text2\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b48311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching embeddings for text1...\n",
      "Fetching embeddings for text2...\n",
      "\n",
      "============================================================\n",
      "OpenAI embedding on MRPC (validation) - Binary Classification\n",
      "============================================================\n",
      "Optimal threshold: 0.840\n",
      "\n",
      "Length of scaled pred similarities: 408\n",
      "\n",
      "Similarity Score Statistics:\n",
      "  Mean similarity (Equivalent pairs):     0.9302\n",
      "  Mean similarity (Non-equivalent pairs): 0.8718\n",
      "  Separation (Difference):                0.0584\n",
      "\n",
      "Classification Metrics:\n",
      "  Accuracy:  0.7598\n",
      "  Precision: 0.7549\n",
      "  Recall:    0.9606\n",
      "  F1-Score:  0.8454\n",
      "\n",
      "Ranking Metrics:\n",
      "  ROC-AUC:   0.7816\n",
      "  PR-AUC:    0.8809\n",
      "============================================================\n",
      "\n",
      "Total execution time: 14.85 seconds (0.25 minutes)\n"
     ]
    }
   ],
   "source": [
    "# @title Evaluate openai model on MRPC dataset\n",
    "# url https://huggingface.co/datasets/SetFit/mrpc\n",
    "\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key from .env file\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file. Please create a .env file with OPENAI_API_KEY=your_key\")\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def batch_get_openai_embeddings(texts, model=\"text-embedding-ada-002\", batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        response = client.embeddings.create(model=model, input=batch)\n",
    "        batch_embeds = [item.embedding for item in response.data]\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return embeddings\n",
    "\n",
    "# Get text pairs and labels\n",
    "text1 = dataset[\"text1\"]\n",
    "text2 = dataset[\"text2\"]\n",
    "labels = dataset[\"label\"]  # Binary labels: 1 = similar, 0 = dissimilar\n",
    "\n",
    "print(\"Fetching embeddings for text1...\")\n",
    "embeds1 = batch_get_openai_embeddings(text1, model=\"text-embedding-3-large\")\n",
    "print(\"Fetching embeddings for text2...\")\n",
    "embeds2 = batch_get_openai_embeddings(text2, model=\"text-embedding-3-large\")\n",
    "\n",
    "# Cosine similarity function\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compute similarity scores (range: -1 to 1)\n",
    "pred_similarities = np.array([\n",
    "    cosine_similarity(e1, e2) for e1, e2 in zip(embeds1, embeds2)\n",
    "])\n",
    "\n",
    "# Scale cosine similarity (-1..1) to (0..1) for easier threshold interpretation\n",
    "pred_similarities_scaled = (pred_similarities + 1) / 2\n",
    "\n",
    "# Find optimal threshold that maximizes F1-score\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    pred_labels = (pred_similarities_scaled >= threshold).astype(int)\n",
    "    f1 = f1_score(labels, pred_labels)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Predictions using optimal threshold\n",
    "pred_labels = (pred_similarities_scaled >= best_threshold).astype(int)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(labels, pred_labels)\n",
    "precision = precision_score(labels, pred_labels)\n",
    "recall = recall_score(labels, pred_labels)\n",
    "f1 = f1_score(labels, pred_labels)\n",
    "\n",
    "# Calculate ROC-AUC and PR-AUC (using scaled similarities as probabilities)\n",
    "roc_auc = roc_auc_score(labels, pred_similarities_scaled)\n",
    "pr_auc = average_precision_score(labels, pred_similarities_scaled)\n",
    "\n",
    "# Calculate mean similarity scores for equivalent and non-equivalent pairs\n",
    "labels_array = np.array(labels)\n",
    "mean_sim_equivalent = np.mean(pred_similarities_scaled[labels_array == 1])\n",
    "mean_sim_non_equivalent = np.mean(pred_similarities_scaled[labels_array == 0])\n",
    "separation = mean_sim_equivalent - mean_sim_non_equivalent\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OpenAI embedding on MRPC (validation) - Binary Classification\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Optimal threshold: {best_threshold:.3f}\")\n",
    "print(f\"\\nLength of scaled pred similarities: {len(pred_similarities_scaled)}\")\n",
    "print(f\"\\nSimilarity Score Statistics:\")\n",
    "print(f\"  Mean similarity (Equivalent pairs):     {mean_sim_equivalent:.4f}\")\n",
    "print(f\"  Mean similarity (Non-equivalent pairs): {mean_sim_non_equivalent:.4f}\")\n",
    "print(f\"  Separation (Difference):                {separation:.4f}\")\n",
    "print(f\"\\nClassification Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"\\nRanking Metrics:\")\n",
    "print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC:    {pr_auc:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# End timing and print duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal execution time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df84105",
   "metadata": {},
   "source": [
    "### Evaluate OPENAI embed model on Clough-Stevenson plagiarism dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876d45cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 95 entries from plagiarism_dataset.jsonl\n",
      "Available categories: ['cut', 'heavy', 'light', 'non']\n"
     ]
    }
   ],
   "source": [
    "# @title Load plagiarism dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "plagiarism_path = Path(\"plagiarism_dataset.jsonl\")\n",
    "if not plagiarism_path.exists():\n",
    "    raise FileNotFoundError(f\"Could not find {plagiarism_path} relative to the notebook directory\")\n",
    "\n",
    "with plagiarism_path.open(\"r\", encoding=\"utf-8\") as fp:\n",
    "    plagiarism_records = [json.loads(line) for line in fp if line.strip()]\n",
    "\n",
    "print(f\"Loaded {len(plagiarism_records)} entries from {plagiarism_path}\")\n",
    "available_categories = sorted({rec[\"paraphrase_category\"] for rec in plagiarism_records})\n",
    "print(f\"Available categories: {available_categories}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6624b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plagiarism_records' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     41\u001b[39m     b = np.array(vec_b)\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m original_texts = [record[\u001b[33m\"\u001b[39m\u001b[33moriginal_task_text\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m \u001b[43mplagiarism_records\u001b[49m]\n\u001b[32m     46\u001b[39m paraphrase_texts = [record[\u001b[33m\"\u001b[39m\u001b[33mparaphrase_text\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m plagiarism_records]\n\u001b[32m     47\u001b[39m categories = [record[\u001b[33m\"\u001b[39m\u001b[33mparaphrase_category\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m plagiarism_records]\n",
      "\u001b[31mNameError\u001b[39m: name 'plagiarism_records' is not defined"
     ]
    }
   ],
   "source": [
    "# @title Compute embeddings and metrics for plagiarism dataset\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file. Please create a .env file with OPENAI_API_KEY=your_key\")\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "MAX_CHARS = 8000\n",
    "\n",
    "\n",
    "def _prepare_text(text: str, max_chars: int = MAX_CHARS) -> str:\n",
    "    text = text.strip()\n",
    "    return text[:max_chars] if len(text) > max_chars else text\n",
    "\n",
    "\n",
    "def embed_texts(texts, model: str = EMBED_MODEL, batch_size: int = 32):\n",
    "    \"\"\"Return embeddings for each text while caching duplicates.\"\"\"\n",
    "    unique_texts = list(dict.fromkeys(texts))\n",
    "    cache = {}\n",
    "    for start in range(0, len(unique_texts), batch_size):\n",
    "        batch_original = unique_texts[start:start + batch_size]\n",
    "        batch_inputs = [_prepare_text(text) for text in batch_original]\n",
    "        response = client.embeddings.create(model=model, input=batch_inputs)\n",
    "        for original_text, embedding in zip(batch_original, response.data):\n",
    "            cache[original_text] = embedding.embedding\n",
    "    return [cache[text] for text in texts]\n",
    "\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    a = np.array(vec_a)\n",
    "    b = np.array(vec_b)\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "\n",
    "original_texts = [record[\"original_task_text\"] for record in plagiarism_records]\n",
    "paraphrase_texts = [record[\"paraphrase_text\"] for record in plagiarism_records]\n",
    "categories = [record[\"paraphrase_category\"] for record in plagiarism_records]\n",
    "\n",
    "print(\"Embedding original passages...\")\n",
    "orig_embeddings = embed_texts(original_texts)\n",
    "print(\"Embedding paraphrases...\")\n",
    "para_embeddings = embed_texts(paraphrase_texts)\n",
    "\n",
    "raw_similarities = np.array([cosine_similarity(o, p) for o, p in zip(orig_embeddings, para_embeddings)])\n",
    "scaled_similarities = (raw_similarities + 1) / 2  # map from (-1, 1) to (0, 1)\n",
    "\n",
    "for record, raw_sim, scaled_sim in zip(plagiarism_records, raw_similarities, scaled_similarities):\n",
    "    record[\"raw_similarity\"] = raw_sim\n",
    "    record[\"scaled_similarity\"] = scaled_sim\n",
    "\n",
    "category_order = [\"non\", \"light\", \"heavy\", \"cut\"]\n",
    "category_scores = defaultdict(list)\n",
    "for record in plagiarism_records:\n",
    "    category_scores[record[\"paraphrase_category\"]].append(record[\"scaled_similarity\"])\n",
    "\n",
    "summary_rows = []\n",
    "reference_mean = None\n",
    "for category in category_order:\n",
    "    scores = np.array(category_scores.get(category, []))\n",
    "    if len(scores) == 0:\n",
    "        continue\n",
    "    mean_score = scores.mean()\n",
    "    if category == \"non\":\n",
    "        reference_mean = mean_score\n",
    "    summary_rows.append({\n",
    "        \"category\": category,\n",
    "        \"count\": len(scores),\n",
    "        \"mean\": mean_score,\n",
    "        \"median\": float(np.median(scores)),\n",
    "        \"std\": float(scores.std(ddof=0)),\n",
    "        \"min\": float(scores.min()),\n",
    "        \"max\": float(scores.max()),\n",
    "    })\n",
    "\n",
    "if reference_mean is not None:\n",
    "    for row in summary_rows:\n",
    "        row[\"delta_vs_non\"] = row[\"mean\"] - reference_mean\n",
    "\n",
    "print(\"\\nCategory-level similarity summary (scaled cosine in [0, 1]):\")\n",
    "for row in summary_rows:\n",
    "    delta = row.get(\"delta_vs_non\", 0.0)\n",
    "    delta_str = f\"{delta:+.4f}\" if row[\"category\"] != \"non\" else \"n/a\"\n",
    "    print(\n",
    "        f\"- {row['category']:>5} | n={row['count']:>2} | mean={row['mean']:.4f} | \"\n",
    "        f\"median={row['median']:.4f} | std={row['std']:.4f} | min={row['min']:.4f} | \"\n",
    "        f\"max={row['max']:.4f} | Δ vs non = {delta_str}\"\n",
    "    )\n",
    "\n",
    "severity_map = {\"non\": 0, \"light\": 1, \"heavy\": 2, \"cut\": 3}\n",
    "severity_scores = np.array([severity_map[cat] for cat in categories])\n",
    "spearman_corr, spearman_p = spearmanr(severity_scores, scaled_similarities)\n",
    "print(\n",
    "    f\"\\nSpearman correlation between similarity score and plagiarism severity: \"\n",
    "    f\"{spearman_corr:.4f} (p={spearman_p:.4f})\"\n",
    ")\n",
    "\n",
    "binary_labels = np.array([1 if cat != \"non\" else 0 for cat in categories])\n",
    "roc_auc = roc_auc_score(binary_labels, scaled_similarities)\n",
    "print(f\"Binary detection ROC-AUC (non vs plagiarized): {roc_auc:.4f}\")\n",
    "\n",
    "best_metrics = None\n",
    "for threshold in np.arange(0.0, 1.01, 0.01):\n",
    "    preds = (scaled_similarities >= threshold).astype(int)\n",
    "    acc = accuracy_score(binary_labels, preds)\n",
    "    tp = int(((preds == 1) & (binary_labels == 1)).sum())\n",
    "    fp = int(((preds == 1) & (binary_labels == 0)).sum())\n",
    "    fn = int(((preds == 0) & (binary_labels == 1)).sum())\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0\n",
    "    if not best_metrics or f1 > best_metrics[\"f1\"]:\n",
    "        best_metrics = {\n",
    "            \"threshold\": threshold,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "if best_metrics is None:\n",
    "    raise RuntimeError(\"Unable to compute threshold-based metrics\")\n",
    "\n",
    "print(\n",
    "    \"Best plagiarism threshold (maximize F1): \"\n",
    "    f\"{best_metrics['threshold']:.2f} -> accuracy={best_metrics['accuracy']:.4f}, \"\n",
    "    f\"precision={best_metrics['precision']:.4f}, recall={best_metrics['recall']:.4f}, \"\n",
    "    f\"F1={best_metrics['f1']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91367e69",
   "metadata": {},
   "source": [
    "The metrics above capture:\n",
    "\n",
    "- **Mean/median similarity per category** (non, light, heavy, cut) along with spread and min/max values, plus the delta relative to the \"non\" baseline.\n",
    "- **Spearman correlation** between the ordinal plagiarism severity labels (`non < light < heavy < cut`) and the scaled cosine similarity, which shows how well the embedding-based similarity tracks the ground-truth ordering.\n",
    "- **Binary plagiarism detection quality** by collapsing the task into `non` vs `plagiarized` (others), reported via ROC-AUC.\n",
    "- **Best-performing similarity threshold** (sweeping 0.00–1.00) that maximizes F1, together with accuracy/precision/recall at that operating point.\n",
    "\n",
    "Re-running the cells will refresh the summary for any updated embeddings, models, or dataset variants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c09f1",
   "metadata": {},
   "source": [
    "#### Similarity score distributions by paraphrase category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92c5db5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# @title Plot smoothed similarity histograms for each category\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde\n\u001b[32m      5\u001b[39m x_grid = np.linspace(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m400\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# @title Plot smoothed similarity histograms for each category\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "x_grid = np.linspace(0, 1, 400)\n",
    "category_colors = {\n",
    "    \"non\": \"#4daf4a\",\n",
    "    \"light\": \"#377eb8\",\n",
    "    \"heavy\": \"#ff7f00\",\n",
    "    \"cut\": \"#e41a1c\",\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for category in category_order:\n",
    "    scores = np.array(category_scores.get(category, []))\n",
    "    if len(scores) == 0:\n",
    "        continue\n",
    "    # KDE needs at least two data points with non-zero variance\n",
    "    if len(scores) >= 2 and scores.std() > 0:\n",
    "        kde = gaussian_kde(scores)\n",
    "        density = kde(x_grid)\n",
    "    else:\n",
    "        # Fallback to a very narrow normal centered on the single value\n",
    "        bandwidth = 0.01\n",
    "        density = np.exp(-0.5 * ((x_grid - scores.mean()) / bandwidth) ** 2)\n",
    "        density /= density.max()\n",
    "    plt.plot(x_grid, density, label=f\"{category} (n={len(scores)})\", color=category_colors.get(category))\n",
    "\n",
    "plt.title(\"Smoothed similarity distributions by category\")\n",
    "plt.xlabel(\"Scaled cosine similarity\")\n",
    "plt.ylabel(\"Density (arbitrary units)\")\n",
    "plt.xlim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebecb00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (text-sim)",
   "language": "python",
   "name": "text-sim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
