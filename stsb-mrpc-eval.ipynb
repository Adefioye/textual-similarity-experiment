{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c847f6a1",
   "metadata": {},
   "source": [
    "## Text Similarity Feature\n",
    "\n",
    "The goal here is to check the performance of openai and cohere embedding model on some specific datasets on \n",
    "textual similarity and to detect plagiarism.\n",
    "\n",
    "### Reporting results\n",
    "#### STSB dataset (1500 datasets. Very short sentences)\n",
    "> Note: Time eval not that accurate as it includes time it takes for dataset to run. I made sure to restart kernel between runs.\n",
    "\n",
    "__text-embedding-3-small__\n",
    "\n",
    "time = 38secs (0.63mins); correlation = 0.88\n",
    "\n",
    "__text-embedding-3-large__\n",
    "\n",
    "time = 34secs (0.57mins); correlation = 8.88\n",
    "\n",
    "#### MRPC dataset (408 datasets. somewhat short sentences as well)\n",
    ">Note this eval does not include time required to load dataset\n",
    "\n",
    "After eyeballing the MRPC dataset, it is a really tricky dataset because any model that really does well on the dataset would have to be good at making a really good distinction between paraphrased entry(in that text2 is a paraphrased text that stays on topic) and semantic entry(in that text2 is a semantic text that good number of words in texts but generally does not stay on topic).\n",
    "\n",
    "##### text-embedding-3-small\n",
    "\n",
    "Scaled similarity embeddings have length 408 same as dataset. hence each score represents an entry in the dataset.\n",
    "\n",
    "time = 11.3 secs (0.19 minutes); \n",
    "mean-similarity (equivalent pair) = 0.93\n",
    "mean-similarity (non-equivalent pair) = 0.87\n",
    "separation between the two = 0.06\n",
    "\n",
    "**classification metrics**\n",
    "precision = 0.7319\n",
    "recall = 0.9785\n",
    "f1-Score = 0.8374\n",
    "\n",
    "**ranking metrics**\n",
    "ROC-AUC = 0.7711\n",
    "PR-AUC = 0.8755\n",
    "\n",
    "##### text-embedding-3-large\n",
    "\n",
    "Scaled similarity embeddings have length 408 same as dataset. hence each score represents an entry in the dataset.\n",
    "\n",
    "time = 14.85 seconds (0.25 minutes); \n",
    "mean-similarity (equivalent pair) = 0.93\n",
    "mean-similarity (non-equivalent pair) = 0.87\n",
    "separation between the two = 0.06\n",
    "\n",
    "**classification metrics**\n",
    "precision = 0.7549\n",
    "recall = 0.9606\n",
    "f1-Score = 0.8454\n",
    "\n",
    "**ranking metrics**\n",
    "ROC-AUC = 0.7816\n",
    "PR-AUC = 0.8809\n",
    "\n",
    "### Summary\n",
    "- For STSB, openai embedding small and large achieved a correlation of `0.88` with ground-truth labels on STSB dataset with no clear winner.\n",
    "- For MRPC, openai embedding small and large have about the same scores. So going forward. There is not much benefit testing on `text-embedding-3-large`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f3ea5",
   "metadata": {},
   "source": [
    "## Evaluate OPENAI embed model on STSB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b17ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load STSB dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load STSB validation set for a quick evaluation\n",
    "dataset = load_dataset(\"sentence-transformers/stsb\", split=\"validation\")\n",
    "\n",
    "# The validation set has 1500 pairs\n",
    "\n",
    "# Get all pairs and true scores\n",
    "sents1 = dataset[\"sentence1\"]\n",
    "sents2 = dataset[\"sentence2\"]\n",
    "scores = dataset[\"score\"]  # Ground truth similarity scores (float: 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9e327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching embeddings for sentence1...\n",
      "Fetching embeddings for sentence2...\n",
      "OpenAI embedding on STSB (validation): Spearman correlation = 0.8775\n",
      "\n",
      "Total execution time: 33.93 seconds (0.57 minutes)\n"
     ]
    }
   ],
   "source": [
    "# @title Evaluate openai model on STSB dataset on huggingface\n",
    "\n",
    "# Evaluate openai embedding on STSB dataset on huggingface\n",
    "# Url https://huggingface.co/datasets/sentence-transformers/stsb\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key from .env file\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file. Please create a .env file with OPENAI_API_KEY=your_key\")\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    # Handles too-long texts by truncating\n",
    "    if len(text) > 8191:\n",
    "        text = text[:8191]\n",
    "    response = client.embeddings.create(model=model, input=[text])\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def batch_get_openai_embeddings(texts, model=\"text-embedding-ada-002\", batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        response = client.embeddings.create(model=model, input=batch)\n",
    "        batch_embeds = [item.embedding for item in response.data]\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "print(\"Fetching embeddings for sentence1...\")\n",
    "embeds1 = batch_get_openai_embeddings(sents1, model=\"text-embedding-3-large\")\n",
    "print(\"Fetching embeddings for sentence2...\")\n",
    "embeds2 = batch_get_openai_embeddings(sents2, model=\"text-embedding-3-large\")\n",
    "\n",
    "# Cosine similarity function\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compute similarity scores\n",
    "pred_similarities = [\n",
    "    cosine_similarity(e1, e2) for e1, e2 in zip(embeds1, embeds2)\n",
    "]\n",
    "# Scale cosine similarity (-1..1) to (0..1) to match the normalized STSB scoring\n",
    "pred_similarities_scaled = [(sim + 1) / 2 for sim in pred_similarities]\n",
    "\n",
    "# Evaluate with Spearman correlation coefficient\n",
    "spearman_corr, _ = spearmanr(pred_similarities_scaled, scores)\n",
    "print(f\"OpenAI embedding on STSB (validation): Spearman correlation = {spearman_corr:.4f}\")\n",
    "\n",
    "# End timing and print duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal execution time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6052998",
   "metadata": {},
   "source": [
    "## Evaluate OPENAI embed model on MRPC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0402c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Load MRPC dataset\n",
    "# url https://huggingface.co/datasets/SetFit/mrpc\n",
    "from datasets import load_dataset\n",
    "\n",
    "# The validation set has 408 pairs\n",
    "dataset = load_dataset(\"SetFit/mrpc\", split=\"validation\")\n",
    "\n",
    "# There is no score here only binary label (`0` and `1`)\n",
    "# 1 means equivalent(SIMILAR)\n",
    "# 0 means non-equivalent(DISSIMILAR)\n",
    "# The two texts are labeled text1 and text2\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b48311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching embeddings for text1...\n",
      "Fetching embeddings for text2...\n",
      "\n",
      "============================================================\n",
      "OpenAI embedding on MRPC (validation) - Binary Classification\n",
      "============================================================\n",
      "Optimal threshold: 0.840\n",
      "\n",
      "Length of scaled pred similarities: 408\n",
      "\n",
      "Similarity Score Statistics:\n",
      "  Mean similarity (Equivalent pairs):     0.9302\n",
      "  Mean similarity (Non-equivalent pairs): 0.8718\n",
      "  Separation (Difference):                0.0584\n",
      "\n",
      "Classification Metrics:\n",
      "  Accuracy:  0.7598\n",
      "  Precision: 0.7549\n",
      "  Recall:    0.9606\n",
      "  F1-Score:  0.8454\n",
      "\n",
      "Ranking Metrics:\n",
      "  ROC-AUC:   0.7816\n",
      "  PR-AUC:    0.8809\n",
      "============================================================\n",
      "\n",
      "Total execution time: 14.85 seconds (0.25 minutes)\n"
     ]
    }
   ],
   "source": [
    "# @title Evaluate openai model on MRPC dataset\n",
    "# url https://huggingface.co/datasets/SetFit/mrpc\n",
    "\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key from .env file\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file. Please create a .env file with OPENAI_API_KEY=your_key\")\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def batch_get_openai_embeddings(texts, model=\"text-embedding-ada-002\", batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        response = client.embeddings.create(model=model, input=batch)\n",
    "        batch_embeds = [item.embedding for item in response.data]\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return embeddings\n",
    "\n",
    "# Get text pairs and labels\n",
    "text1 = dataset[\"text1\"]\n",
    "text2 = dataset[\"text2\"]\n",
    "labels = dataset[\"label\"]  # Binary labels: 1 = similar, 0 = dissimilar\n",
    "\n",
    "print(\"Fetching embeddings for text1...\")\n",
    "embeds1 = batch_get_openai_embeddings(text1, model=\"text-embedding-3-large\")\n",
    "print(\"Fetching embeddings for text2...\")\n",
    "embeds2 = batch_get_openai_embeddings(text2, model=\"text-embedding-3-large\")\n",
    "\n",
    "# Cosine similarity function\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compute similarity scores (range: -1 to 1)\n",
    "pred_similarities = np.array([\n",
    "    cosine_similarity(e1, e2) for e1, e2 in zip(embeds1, embeds2)\n",
    "])\n",
    "\n",
    "# Scale cosine similarity (-1..1) to (0..1) for easier threshold interpretation\n",
    "pred_similarities_scaled = (pred_similarities + 1) / 2\n",
    "\n",
    "# Find optimal threshold that maximizes F1-score\n",
    "best_threshold = 0.5\n",
    "best_f1 = 0\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    pred_labels = (pred_similarities_scaled >= threshold).astype(int)\n",
    "    f1 = f1_score(labels, pred_labels)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Predictions using optimal threshold\n",
    "pred_labels = (pred_similarities_scaled >= best_threshold).astype(int)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(labels, pred_labels)\n",
    "precision = precision_score(labels, pred_labels)\n",
    "recall = recall_score(labels, pred_labels)\n",
    "f1 = f1_score(labels, pred_labels)\n",
    "\n",
    "# Calculate ROC-AUC and PR-AUC (using scaled similarities as probabilities)\n",
    "roc_auc = roc_auc_score(labels, pred_similarities_scaled)\n",
    "pr_auc = average_precision_score(labels, pred_similarities_scaled)\n",
    "\n",
    "# Calculate mean similarity scores for equivalent and non-equivalent pairs\n",
    "labels_array = np.array(labels)\n",
    "mean_sim_equivalent = np.mean(pred_similarities_scaled[labels_array == 1])\n",
    "mean_sim_non_equivalent = np.mean(pred_similarities_scaled[labels_array == 0])\n",
    "separation = mean_sim_equivalent - mean_sim_non_equivalent\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OpenAI embedding on MRPC (validation) - Binary Classification\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Optimal threshold: {best_threshold:.3f}\")\n",
    "print(f\"\\nLength of scaled pred similarities: {len(pred_similarities_scaled)}\")\n",
    "print(f\"\\nSimilarity Score Statistics:\")\n",
    "print(f\"  Mean similarity (Equivalent pairs):     {mean_sim_equivalent:.4f}\")\n",
    "print(f\"  Mean similarity (Non-equivalent pairs): {mean_sim_non_equivalent:.4f}\")\n",
    "print(f\"  Separation (Difference):                {separation:.4f}\")\n",
    "print(f\"\\nClassification Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"\\nRanking Metrics:\")\n",
    "print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"  PR-AUC:    {pr_auc:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# End timing and print duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal execution time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b30607",
   "metadata": {},
   "source": [
    "### Evaluate cohere model on STSB dataset on huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c44a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Cohere embedding on STSB dataset (validation split)\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "assert COHERE_API_KEY is not None, \"Set COHERE_API_KEY in your environment.\"\n",
    "\n",
    "# Models to try 'embed-english-v3.0', 'embed-english-light-v3.0'.\n",
    "def batch_get_cohere_embeddings(texts, model=\"embed-english-v3.0\", batch_size=32):\n",
    "    endpoint = \"https://api.cohere.ai/v2/embed\"\n",
    "    headers = {\"Authorization\": f\"Bearer {COHERE_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Cohere Embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        data = {\n",
    "            \"texts\": batch_texts,\n",
    "            \"model\": model,\n",
    "            \"input_type\": \"classification\"\n",
    "        }\n",
    "        response = requests.post(endpoint, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        batch_embeds = response.json()[\"embeddings\"]\n",
    "        embeddings.extend(batch_embeds)\n",
    "    return embeddings\n",
    "\n",
    "print(\"Fetching Cohere embeddings for sentence1...\")\n",
    "cohere_embeds1 = batch_get_cohere_embeddings(sents1)\n",
    "print(\"Fetching Cohere embeddings for sentence2...\")\n",
    "cohere_embeds2 = batch_get_cohere_embeddings(sents2)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "cohere_pred_similarities = [\n",
    "    cosine_similarity(e1, e2) for e1, e2 in zip(cohere_embeds1, cohere_embeds2)\n",
    "]\n",
    "cohere_pred_similarities_scaled = [(sim + 1) / 2 for sim in cohere_pred_similarities]\n",
    "\n",
    "cohere_spearman_corr, _ = spearmanr(cohere_pred_similarities_scaled, scores)\n",
    "print(f\"Cohere embedding on STSB (validation): Spearman correlation = {cohere_spearman_corr:.4f}\")\n",
    "\n",
    "# End timing and print duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal execution time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (text-sim)",
   "language": "python",
   "name": "text-sim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
